\section{Introduction}

Advancements in MRI and neuroimaging technology have enabled researchers to begin to understand the mechanisms of healthy brain development \cite{giedd1999brain} and neurological disorders, such as multiple sclerosis \cite{bakshi2008mri}. Due to the large variability of brain morphology in the population, increasingly large sample sizes are needed to answer important/interesting biomedical questions. In order to make sense of all the information, automated algorithms were developed to reduce information-rich 3D MRI images to 1 dimensional summary metrics (such as total brain volume) that are easy to understand. Automated algorithms save a lot of time compared to manual human inspection, but lack the advanced visual system of humans. As a result, they often make systematic errors, especially for brains with pathology or early in development. I think that data science can help answer neuroscience research questions using a crowd-sourcing strategy, where errors from automatic quantification can be resolved efficiently by non-expert "citizen-scientists".

Crowd sourcing has been successful in many other disciplines \cite{wiggins2011conservation}, including mathematics \cite{cranshaw2011polymath}, astronomy \cite{lintott2008galaxy}, and biochemistry\cite{eiben2012increased}. Recently, over 200,000 "citizen-neuroscientists"  from over 147 countries, helped identify neuronal connections in a mouse retina, through the Eyewire game \cite{kim2014space}. This crowd-sourced game led to a new understanding of how mammalian retinal cells detect motion. I propose to implement 3 key features of the EyeWire paradigm to adapt it for multiple neuroimaging use cases. First, by breaking up the problem into smaller "micro-tasks", scientists are able to access a much larger user-pool of non-experts \cite{kittur2008crowdsourcing}. Second, machine learning algorithms were trained to help with the task, which improved the speed of manual neuronal tracing and validated non-expert input. Lastly, EyeWire transformed a dull, monotonous task for experts into a fun, competitive game that trained non-experts and acquired valuable scientific data. I propose to implement these features to create an open-source platform for crowd-sourcing neuroimaging research.


\begin{itemize}

\item I propose an open platform that can be adapted to multiple use-cases, particularly: different tissue classification applications, such as brain extraction, and lesion segmentation. 
\end{itemize}

\section{Specific Aims}
\begin{enumerate}
\item Scaleablity and Security: A scaleable database system and server backend that keeps data private by piece-wise exposure
\item Learning by Example: Machine learning tool that learns from human curation
\item Training and Gamification: User interface that trains users to solve a specific problem to solve, and keeps them engaged to keep solving
\\
\\
The system I propose can be generalized to many use cases, including MS lesion segmentation, but for this proposal, these aims will be applied to a specific use case, which is brain extraction. Brain extraction is important because brain volume is an important feature to study in human development and aging, but also in diseases like MS, and in genotype-phenotype studies, where precision is important due to small effect sizes. Brain extraction is difficult for a computer alone the intensity of the dura looks similar to that of gray matter, and often require manual editing, as described in the Freesurfer package. The data used in this proposal will be open-source data, but the platform can accommodate private data as well, due to aim 1. 

\end{enumerate}

\section{Specific Aim 1: Scaleablility and Security}
EyeWire game had over 200,000 users, so our solution needs to scale well. 
We will address 2 key challenges:
\begin{itemize}
\item data management, handling large datasets, distributed computing 
\begin{itemize}
\item need to learn how to use Amazon, spark, distributed computing. Resources at the eScience institute will help. 
\item learn how to connect to amazon turk for paid incentive, zooniverse for free
\end{itemize}
\item serving piece-wise "chunks" of data rather than the whole dataset. This preserves privacy if the data is not open, and reduces the fatigue of users. Need to serve intelligently to figure out which pieces to serve next.
\item show different views: axial, coronal, sagittal
\begin{itemize}
\item different datasets will need to be chunked differently. Lesions need all 3 slices view but brain mask editing, only 1. Needs to be user-specified, to answer the question "how much chunking is necessary for this application"
\end{itemize}
\end{itemize}

\section{Specific Aim 2: Learning by Example}

Address 2 challenges:
\begin{itemize}
\item resolving user input to create a final image, based on training data
\begin{itemize}
\item create a probability image weighted by how well each user did on the training data
\item joint fusion strategy?
\end{itemize}
\item predicting regions of variability to serve more efficiently
\begin{itemize}
\item can we predict which voxels are "controversial" and only show those?
\end{itemize}
\end{itemize}

\section{Specific Aim 3: Training and Gamification}
\begin{itemize}
\item training: how frequently does training data need to be used
\begin{itemize}
\item need to signal to users that this is being verified, avoid malicious users, increase time and care spent on task
\item also keep track of time spent on task. If many voxels were erased, were they erased quickly or slowly. 
\end{itemize}
\item UI edit, gamify
\begin{itemize}
\item already have mindcontrol platform to do editing
\item give badges relative to volume of voxels edited, reward training with points for feedback
\item use amazon turk's api rather than the turk platform itself. 
\end{itemize}
\end{itemize}